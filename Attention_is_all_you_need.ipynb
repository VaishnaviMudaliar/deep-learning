{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN3ETp0d2IgvtGaeS7EDnCM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VaishnaviMudaliar/deep-learning/blob/main/Attention_is_all_you_need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì• Importing PyTorch Modules\n",
        "\n",
        "\n",
        "### ‚úÖ Explanation\n",
        "\n",
        "- **`import torch`**  \n",
        "  This imports the core PyTorch library. It provides the foundational tools for:\n",
        "  - Creating and working with **tensors**\n",
        "  - Performing operations on **GPU/CPU**\n",
        "  - Handling **automatic differentiation** for training neural networks\n",
        "\n",
        "- **`import torch.nn as nn`**  \n",
        "  This imports the `torch.nn` module, which contains classes and functions to help define and train **neural networks**.  \n",
        "  Common components include:\n",
        "  - Layers like `nn.Linear`, `nn.Conv2d`, etc.\n",
        "  - Activation functions like `nn.ReLU`, `nn.Sigmoid`, etc.\n",
        "  - Loss functions like `nn.CrossEntropyLoss`, `nn.MSELoss`, etc.\n",
        "  - The base class `nn.Module` to create custom models\n",
        "\n",
        "By importing it as `nn`, we can easily write shorter code such as `nn.Linear(...)` instead of `torch.nn.Linear(...)`.\n"
      ],
      "metadata": {
        "id": "laze2SHwYRCs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3PRrN3sNIosN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Self-Attention Mechanism (Multi-Headed)\n",
        "\n",
        "```python\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "```\n",
        "\n",
        "### üîß `__init__` Method Explanation\n",
        "\n",
        "- `embed_size`: Total dimensionality of the input embeddings.\n",
        "- `heads`: Number of attention heads (multi-head attention splits the work into parallel \"heads\").\n",
        "- `head_dim`: Size of each attention head = `embed_size // heads`.\n",
        "\n",
        "- Three linear layers for computing **values**, **keys**, and **queries**, applied per head.\n",
        "- `fc_out`: Final linear layer to combine all heads' output back into a single embedding.\n",
        "\n",
        "> üìå The `assert` ensures that `embed_size` is cleanly divisible by `heads`.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        energy = torch.einsum(\"nqhd, nkhd -> nhqk\", [queries, keys])\n",
        "```\n",
        "\n",
        "### üîÑ `forward` Method Explanation\n",
        "\n",
        "- Inputs:\n",
        "  - `values`, `keys`, `query`: Tensors with embedding data (shape: `[batch_size, seq_len, embed_size]`)\n",
        "  - `mask`: Optional tensor to mask out positions in the sequence\n",
        "\n",
        "- Reshaping inputs to split across attention heads:\n",
        "  - New shape: `(batch_size, seq_len, heads, head_dim)`\n",
        "\n",
        "- Applying linear projections to get keys, queries, and values.\n",
        "\n",
        "- **Energy calculation**:\n",
        "  - Using `torch.einsum(\"nqhd, nkhd -> nhqk\", ...)` to compute dot-product attention scores.\n",
        "  - Resulting shape: `(batch_size, heads, query_len, key_len)`\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
        "```\n",
        "\n",
        "### üé≠ Masking and Softmax\n",
        "\n",
        "- If a mask is provided, attention to those positions is heavily penalized by setting their energy scores to a large negative number.\n",
        "- Apply **softmax** over the `key_len` dimension to get attention weights.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "        out = torch.einsum(\"nhql,nlhd -> nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "```\n",
        "\n",
        "### üß™ Weighted Sum and Output\n",
        "\n",
        "- Weighted combination of values based on attention scores using another `einsum`.\n",
        "- Output shape after reshaping: `(batch_size, query_len, embed_size)`\n",
        "- Final linear layer `fc_out` combines all heads' outputs into one.\n",
        "\n",
        "---\n",
        "\n",
        "### üîö Summary\n",
        "\n",
        "This `SelfAttention` module implements multi-head scaled dot-product attention. It:\n",
        "- Projects inputs into multiple attention heads\n",
        "- Computes attention scores\n",
        "- Applies softmax with optional masking\n",
        "- Combines outputs from all heads\n",
        "- Projects the result back to the original embedding size\n",
        "\n",
        "> üß† This is the core of the **Transformer** architecture!\n"
      ],
      "metadata": {
        "id": "5KZC2wPcYZRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embed_size, heads):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.heads = heads\n",
        "    self.head_dim = embed_size // heads\n",
        "    assert(self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
        "    self.values = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "    self.keys = nn.Linear(self.head_dim, self.head_dim, bias = False )\n",
        "    self.queries = nn.Linear(self.head_dim, self.head_dim, bias = False )\n",
        "    self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
        "\n",
        "  def forward(self, values, keys, query, mask):\n",
        "    N = query.shape[0]\n",
        "    value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "    #Split embedddings into self.heads pieces\n",
        "\n",
        "    values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "    keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "    queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "    values = self.values(values)\n",
        "    keys = self.keys(keys)\n",
        "    queries = self.queries(queries)\n",
        "    energy = torch.einsum(\"nqhd, nkhd -> nhqk\" , [queries,keys])\n",
        "\n",
        "    # queries shape : (N, query_len, heads, head_dim)\n",
        "    # keys shape : (N, key_len, heads, head_dim)\n",
        "\n",
        "    # energy shape : (N, heads, query_len, key_len)\n",
        "\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "    attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim = 3)\n",
        "\n",
        "    out = torch.einsum(\"nhql,nlhd -> nqhd\", [attention, values]).reshape(N, query_len, self.heads*self.head_dim)\n",
        "\n",
        "    # attention shape: (N, heads, query_len, key_len)\n",
        "    # values shape: (N, value_len, heads, head_dim)\n",
        "    # afer einsum (N, query_len, heads, head_dim) then flatten last two dimensions\n",
        "\n",
        "    out = self.fc_out(out)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "alW6ckSCLfsp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîó Transformer Block\n",
        "\n",
        "This class implements a **single block of the Transformer encoder** architecture, combining self-attention and a feed-forward network with residual connections, layer normalization, and dropout.\n",
        "\n",
        "```python\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "```\n",
        "\n",
        "### üß± `__init__` Constructor\n",
        "\n",
        "- `embed_size`: Dimensionality of token embeddings\n",
        "- `heads`: Number of self-attention heads\n",
        "- `dropout`: Dropout probability to prevent overfitting\n",
        "- `forward_expansion`: Multiplier for expanding the hidden layer in the feed-forward network\n",
        "\n",
        "**Main Components:**\n",
        "\n",
        "- `SelfAttention(...)`: Multi-head self-attention layer\n",
        "- `LayerNorm(...)`: Layer normalization for stabilizing and speeding up training\n",
        "- `feed_forward`: Two-layer MLP with ReLU activation\n",
        "- `Dropout(...)`: Regularization to prevent overfitting\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "\n",
        "        return out\n",
        "```\n",
        "\n",
        "### üîÑ `forward` Method\n",
        "\n",
        "1. **Self-Attention + Residual + LayerNorm**\n",
        "   - Computes attention over the input sequence.\n",
        "   - Adds the original `query` (residual connection).\n",
        "   - Applies `LayerNorm` and `Dropout`.\n",
        "\n",
        "2. **Feed-Forward Network + Residual + LayerNorm**\n",
        "   - Passes through a 2-layer MLP (`feed_forward`).\n",
        "   - Adds the residual input `x`.\n",
        "   - Applies another `LayerNorm` and `Dropout`.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary\n",
        "\n",
        "A `TransformerBlock` combines:\n",
        "\n",
        "- üß≤ **Multi-head self-attention** (focuses on relevant parts of input)\n",
        "- üîÅ **Residual connections** (helps gradient flow)\n",
        "- üß™ **Layer normalization** (improves training stability)\n",
        "- üöø **Dropout** (regularization)\n",
        "- üîß **Feed-forward network** (non-linearity and learning capacity)\n",
        "\n",
        "> üìå Multiple `TransformerBlock`s are stacked in Transformer encoders to build deep and powerful models.\n"
      ],
      "metadata": {
        "id": "Ps9asq1XY0f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embed_size, heads, dropout, forward_expansion) :\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.attention = SelfAttention(embed_size, heads)\n",
        "    self.norm1 = nn.LayerNorm(embed_size)\n",
        "    self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(embed_size, forward_expansion*embed_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(forward_expansion*embed_size, embed_size),\n",
        "\n",
        "\n",
        "    )\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, value, key, query, mask):\n",
        "    attention = self.attention(value, key, query, mask)\n",
        "    x = self.dropout(self.norm1(attention + query))\n",
        "\n",
        "    forward = self.feed_forward(x)\n",
        "    out = self.dropout(self.norm2(forward + x))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mG20fsn70MzQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìò Transformer Encoder\n",
        "\n",
        "This class implements the **encoder** component of the Transformer model. It converts a sequence of input tokens into rich contextual embeddings using multiple layers of self-attention and feed-forward networks.\n",
        "\n",
        "```python\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_size, heads, dropout=dropout, forward_expansion=forward_expansion)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "```\n",
        "\n",
        "### üîß `__init__` Constructor Explanation\n",
        "\n",
        "- **`src_vocab_size`**: Number of tokens in the source vocabulary.\n",
        "- **`embed_size`**: Size of the embedding vectors.\n",
        "- **`num_layers`**: Number of Transformer blocks (depth of encoder).\n",
        "- **`heads`**\n"
      ],
      "metadata": {
        "id": "fYsEuxFIZBr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length, ) :\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.device = device\n",
        "    self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "    self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "    self.layers = nn.ModuleList([\n",
        "        TransformerBlock(embed_size, heads, dropout=dropout, forward_expansion=forward_expansion) for _ in range(num_layers)\n",
        "    ] )\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    N, seq_length = x.shape\n",
        "    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "\n",
        "    out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      out = layer(out, out, out, mask)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LgfGpOmATr3W"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÅ Transformer Decoder Block\n",
        "\n",
        "This class implements a **single decoder block** in the Transformer architecture. A decoder block uses two attention layers:\n",
        "1. **Masked self-attention** on the decoder‚Äôs own inputs (to prevent peeking at future tokens).\n",
        "2. **Encoder-decoder attention** (to attend to encoder outputs).\n",
        "\n",
        "```python\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "```\n",
        "\n",
        "### üß± `__init__` Constructor\n",
        "\n",
        "- **`embed_size`**: Dimensionality of token embeddings.\n",
        "- **`heads`**: Number of attention heads.\n",
        "- **`forward_expansion`**: Multiplier for the hidden dimension in the feed-forward layer.\n",
        "- **`dropout`**: Dropout rate for regularization.\n",
        "- **`device`**: Device to run the model (`cpu` or `cuda`).\n",
        "\n",
        "**Components:**\n",
        "- `SelfAttention(...)`: Used for **masked self-attention**.\n",
        "- `LayerNorm(...)`: Normalizes the output for stability.\n",
        "- `TransformerBlock(...)`: Used for **encoder-decoder attention** followed by feed-forward.\n",
        "- `Dropout(...)`: Adds regularization to avoid overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "    def forward(self, x, value, key, src_mask, trg_mask):\n",
        "        attention = self.attention(x, x, x, trg_mask)\n",
        "        query = self.dropout(self.norm(attention + x))\n",
        "        out = self.transformer_block(value, key, query, src_mask)\n",
        "        return out\n",
        "```\n",
        "\n",
        "### üîÑ `forward` Method Explanation\n",
        "\n",
        "- **Inputs**:\n",
        "  - `x`: Target input embeddings (from previous decoder time steps).\n",
        "  - `value`, `key`: Outputs from the encoder (used in encoder-decoder attention).\n",
        "  - `src_mask`: Mask for the encoder inputs (to ignore padding).\n",
        "  - `trg_mask`: Mask for decoder inputs (to prevent future token access).\n",
        "\n",
        "#### üîπ Step-by-Step:\n",
        "\n",
        "1. **Masked Self-Attention**:\n",
        "   - Applies self-attention over decoder inputs `x`.\n",
        "   - Uses `trg_mask` to block future tokens.\n",
        "   - Residual connection: `attention + x`\n",
        "   - Normalized and regularized.\n",
        "\n",
        "2. **Encoder-Decoder Attention** (via `TransformerBlock`):\n",
        "   - Uses encoder outputs (`key`, `value`) to inform decoding.\n",
        "   - Takes the `query` from masked self-attention.\n",
        "   - Uses `src_mask` to ignore encoder padding tokens.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary\n",
        "\n",
        "A `DecoderBlock`:\n",
        "- Performs **masked self-attention** on target sequence so the model predicts one token at a time.\n",
        "- Uses **encoder-decoder attention** to focus on relevant encoder output.\n",
        "- Passes through a feed-forward layer with residuals and normalization.\n",
        "\n",
        "> üîÑ Multiple decoder blocks are stacked to form the full decoder in a Transformer model (e.g., GPT, translation models).\n"
      ],
      "metadata": {
        "id": "FKgFAHspZJ1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self,embed_size, heads, forward_expansion, dropout, device ):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "\n",
        "    self.attention = SelfAttention(embed_size, heads)\n",
        "    self.norm = nn.LayerNorm(embed_size)\n",
        "    self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x, value, key, src_mask, trg_mask):\n",
        "    attention = self.attention(x, x, x, trg_mask)\n",
        "    query = self.dropout(self.norm(attention + x))\n",
        "    out = self.transformer_block(value, key, query, src_mask)\n",
        "    return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tHhyGowquGcF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìô Transformer Decoder\n",
        "\n",
        "The `Decoder` class implements the full **decoder** component of the Transformer architecture. It consists of multiple decoder blocks that perform masked self-attention, encoder-decoder attention, and feed-forward transformations.\n",
        "\n",
        "```python\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "```\n",
        "\n",
        "### üß± `__init__` Constructor Explanation\n",
        "\n",
        "- **`trg_vocab_size`**: Number of tokens in the target language vocabulary.\n",
        "- **`embed_size`**: Dimension of embeddings.\n",
        "- **`num_layers`**: Number of decoder blocks.\n",
        "- **`heads`**: Number of attention heads in each block.\n",
        "- **`forward_expansion`**: Multiplier for hidden size in feed-forward layers.\n",
        "- **`dropout`**: Dropout rate.\n",
        "- **`device`**: Device to allocate tensors.\n",
        "- **`max_length`**: Maximum length for positional embeddings.\n",
        "\n",
        "**Main Components:**\n",
        "- `word_embedding`: Converts token indices to vectors.\n",
        "- `position_embedding`: Adds sequence position info.\n",
        "- `DecoderBlock`: List of decoder blocks to build depth.\n",
        "- `fc_out`: Final linear layer mapping to vocabulary logits.\n",
        "- `dropout`: Applied to embeddings and intermediate steps.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "\n",
        "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "        return out\n",
        "```\n",
        "\n",
        "### üîÑ `forward` Method Explanation\n",
        "\n",
        "- **Inputs**:\n",
        "  - `x`: Target token indices (`[N, tgt_seq_len]`).\n",
        "  - `enc_out`: Output from the encoder (`[N, src_seq_len, embed_size]`).\n",
        "  - `src_mask`: Mask for encoder input_\n"
      ],
      "metadata": {
        "id": "ac5f5cqqZVT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length) -> None:\n",
        "    super(Decoder, self).__init__()\n",
        "    self.device = device\n",
        "    self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "    self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "    self.layers = nn.ModuleList([\n",
        "        DecoderBlock(embed_size, heads, forward_expansion, dropout, device) for _ in range(num_layers)\n",
        "    ])\n",
        "\n",
        "    self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self,x, enc_out, src_mask, trg_mask): # Corrected indentation\n",
        "    N, seq_length = x.shape\n",
        "    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "    x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
        "\n",
        "    out = self.fc_out(x)\n",
        "    return out"
      ],
      "metadata": {
        "id": "YUGFqHqZEfPv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Transformer Model (Full Architecture)\n",
        "\n",
        "This `Transformer` class brings together the **Encoder** and **Decoder** into a full sequence-to-sequence model, as described in the original [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) paper.\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Constructor: `__init__`\n",
        "\n",
        "```python\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,\n",
        "                 embed_size=256, num_layers=6, forward_expansion=4,\n",
        "                 heads=8, dropout=0, device=\"cpu\", max_length=100):\n",
        "```\n",
        "\n",
        "### üîπ Arguments:\n",
        "\n",
        "- **`src_vocab_size` / `trg_vocab_size`**: Vocabulary sizes for source and target languages.\n",
        "- **`src_pad_idx` / `trg_pad_idx`**: Padding token indices (for masking).\n",
        "- **`embed_size`**: Dimension of token embeddings.\n",
        "- **`num_layers`**: Number of encoder and decoder blocks.\n",
        "- **`heads`**: Number of attention heads.\n",
        "- **`forward_expansion`**: Multiplier for feed-forward hidden size.\n",
        "- **`dropout`**: Dropout rate.\n",
        "- **`device`**: Device (`cpu` or `cuda`).\n",
        "- **`max_length`**: Maximum sequence length.\n",
        "\n",
        "### üß© Components:\n",
        "\n",
        "```python\n",
        "self.encoder = Encoder(...)\n",
        "self.decoder = Decoder(...)\n",
        "self.src_pad_idx = src_pad_idx\n",
        "self.trg_pad_idx = trg_pad_idx\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üõ°Ô∏è Masking Functions\n",
        "\n",
        "### üî∑ `make_src_mask`\n",
        "\n",
        "```python\n",
        "def make_src_mask(self, src):\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    return src_mask.to(self.device)\n",
        "```\n",
        "\n",
        "- Creates a mask for the **source input** to ignore `<PAD>` tokens.\n",
        "- Shape: `(N, 1, 1, src_len)` ‚Äî broadcastable for attention.\n",
        "\n",
        "### üî∂ `make_trg_mask`\n",
        "\n",
        "```python\n",
        "def make_trg_mask(self, trg):\n",
        "    N, trg_len = trg.shape\n",
        "    trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len)\n",
        "    return trg_mask.to(self.device)\n",
        "```\n",
        "\n",
        "- Produces a **look-ahead mask** for the target sequence to prevent attending to future tokens.\n",
        "- Shape: `(N, 1, trg_len, trg_len)`\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ `forward` Pass\n",
        "\n",
        "```python\n",
        "def forward(self, src, trg):\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "    enc_src = self.encoder(src, src_mask)\n",
        "    out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "    return out\n",
        "```\n",
        "\n",
        "### üß† Process Flow:\n",
        "\n",
        "1. Generate masks for both source and target inputs.\n",
        "2. Pass the **source sequence** and mask to the encoder.\n",
        "3. Pass the **target sequence**, encoder output, and both masks to the decoder.\n",
        "4. Return the logits for each token in the target vocabulary.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "This `Transformer` class:\n",
        "- Fully implements the **encoder-decoder Transformer architecture**.\n",
        "- Handles positional embeddings, multi-head attention, masking, and feed-forward layers.\n",
        "- Can be used for tasks like **machine translation**, **text generation**, and more.\n",
        "\n",
        "> üß™ Tip: To train this model, use `nn.CrossEntropyLoss(ignore_index=trg_pad_idx)` and optimize over the output predictions vs. the ground truth target sequence.\n"
      ],
      "metadata": {
        "id": "8l-dFelOZgEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size = 256, num_layers = 6, forward_expansion = 4, heads = 8, dropout = 0, device = \"cpu\", max_length = 100 ):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "        src_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        device,\n",
        "       forward_expansion,\n",
        "       dropout,\n",
        "       max_length)\n",
        "\n",
        "    self.decoder = Decoder(\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        device,\n",
        "        max_length\n",
        "    )\n",
        "\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "\n",
        "  def make_src_mask(self, src):\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    #(N,1,1,src_length)\n",
        "\n",
        "    return src_mask.to(self.device)\n",
        "\n",
        "  def make_trg_mask(self, trg):\n",
        "    N, trg_len = trg.shape\n",
        "    trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len)\n",
        "    return trg_mask.to(self.device)\n",
        "\n",
        "  def forward(self, src, trg):\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    trg_mask = self.make_trg_mask(trg)\n",
        "    enc_src = self.encoder(src, src_mask)\n",
        "    out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "    return out\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "pHh_keftKL3f"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model Forward Pass Example\n",
        "\n",
        "This section demonstrates how the **Transformer model** works with source and target sequences for a typical sequence-to-sequence task. We'll walk through the forward pass using example input tensors.\n",
        "\n",
        "## üîß Code Overview\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "# Define input tensors (source and target sequences)\n",
        "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n",
        "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
        "\n",
        "# Vocabulary and padding indices\n",
        "src_pad_idx = 0\n",
        "trg_pad_idx = 0\n",
        "src_vocab_size = 10\n",
        "trg_vocab_size = 10\n",
        "\n",
        "# Clamping values to ensure they are within the valid range for vocab indices\n",
        "x = torch.clamp(x, 0, src_vocab_size - 1)\n",
        "trg = torch.clamp(trg, 0, trg_vocab_size - 1)\n",
        "\n",
        "# Initialize the model\n",
        "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)\n",
        "\n",
        "# Forward pass (using trg[:-1] for target sequence input)\n",
        "out = model(x, trg[:,:-1])\n",
        "\n",
        "# Output the shape of the result\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "## üí° Explanation\n",
        "\n",
        "### 1. **Input Tensors: `x` and `trg`**\n",
        "- **`x` (source sequence)**:\n",
        "  - Shape: `(2, 9)` ‚Äî 2 sequences, each with 9 tokens.\n",
        "- **`trg` (target sequence)**:\n",
        "  - Shape: `(2, 8)` ‚Äî 2 sequences, each with 8 tokens.\n",
        "\n",
        "These sequences represent indices in the source and target vocabularies.\n",
        "\n",
        "### 2. **Clamping Values**\n",
        "- The values in `x` and `trg` are clamped to ensure that all indices are valid with respect to the vocabulary sizes (`src_vocab_size` and `trg_vocab_size`).\n",
        "- This prevents any indices from exceeding the bounds of the vocabulary, ensuring that all values lie within the valid range `[0, vocab_size - 1]`.\n",
        "\n",
        "### 3. **Transformer Model Initialization**\n",
        "The model is instantiated with the following key parameters:\n",
        "- **`src_vocab_size = 10`**: Source vocabulary size.\n",
        "- **`trg_vocab_size = 10`**: Target vocabulary size.\n",
        "- **`src_pad_idx = 0` and `trg_pad_idx = 0`**: Padding token indices (0 in this case).\n",
        "- **Device**: Set to `\"cpu\"`, but can be changed to `\"cuda\"` for GPU processing.\n",
        "\n",
        "### 4. **Masking and Forward Pass**\n",
        "- **Masking**: The `make_src_mask` and `make_trg_mask` methods are called inside the `Transformer` model's forward pass to create appropriate masks for the source and target sequences.\n",
        "- **Target Sequence**: `trg[:,:-1]` is used as the input to the decoder. This slicing excludes the last token from the target sequence, as the decoder predicts the next token in the sequence.\n",
        "  \n",
        "### 5. **Expected Output Shape**\n",
        "The output shape from the forward pass will be `(2, 7, 10)`:\n",
        "- **2**: Batch size (2 sequences).\n",
        "- **7**: Length of the target sequence minus the last token (`trg[:,:-1]`).\n",
        "- **10**: The target vocabulary size (`trg_vocab_size`).\n",
        "\n",
        "This shape indicates the decoder's prediction for each token in the target sequence, with probabilities for each token in the vocabulary.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Expected Output\n",
        "\n",
        "```python\n",
        "torch.Size([2, 7, 10])\n",
        "```\n",
        "\n",
        "This shape means that the model has produced predictions for 7 tokens in the target sequence for each of the 2 sequences in the batch, with 10 possible tokens (vocabulary size) at each position in the sequence.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "In summary:\n",
        "- The code demonstrates a forward pass through the **Transformer model**.\n",
        "- It uses source and target sequences with appropriate masking and clamping to ensure valid indices.\n",
        "- The final output shape reflects the model's predictions for the target sequence, considering the given input and vocabulary sizes.\n",
        "\n",
        "> üßë‚Äçüè´ **Tip**: Ensure that your target sequences are properly shifted by 1 token when feeding them into the decoder, as is done with `trg[:,:-1]` here.\n"
      ],
      "metadata": {
        "id": "ixl5plYHaBaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "\n",
        "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n",
        "\n",
        "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
        "\n",
        "src_pad_idx = 0\n",
        "trg_pad_idx = 0\n",
        "src_vocab_size = 10\n",
        "trg_vocab_size = 10\n",
        "\n",
        "# The error likely occurs because src_vocab_size and trg_vocab_size are defined as 10,\n",
        "# but 'x' and 'trg' may contain values greater than 9.\n",
        "# The following lines will clamp 'x' and 'trg' values to be within the vocabulary size.\n",
        "x = torch.clamp(x, 0, src_vocab_size - 1)\n",
        "trg = torch.clamp(trg, 0, trg_vocab_size - 1)\n",
        "\n",
        "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)\n",
        "\n",
        "out = model(x, trg[:,:-1])\n",
        "\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvHetVfrQNim",
        "outputId": "7c2ec9fc-d927-43be-bb6a-13da274ccdcb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 7, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3JR1PrSMUv3e"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}